{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.logistic回归\n",
    "逻辑回归中，预测值:\n",
    "\n",
    "$$h = P(y=1|x)$$\n",
    "\n",
    "其表示值为1的概率，取值范围在[0,1]之间\n",
    "引入sigmoid函数，预测值:\n",
    "\n",
    "$$h = Sigmoid(w^{T}x + b) = \\sigma(w^{T}x + b)$$\n",
    "\n",
    "其中\n",
    "$$Sigmoid(z) = \\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "注意：函数的一阶导数可以用其自身表示:\n",
    "$$\\frac{\\partial \\sigma(z)}{\\partial z} = \\sigma(z)(1-\\sigma(z))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.logistic回归 损失函数\n",
    "#### Loss function\n",
    "一般来说，使用平方错误来衡量loss function:\n",
    "$$L(h^{hat},y) = \\frac{1}{2}(y^{hat}-y)^{2}$$\n",
    "但是，对于logistic 回归来说，一般不适合用平方错误来作为loss function,这是因为上面的平方错误损失函数一般是非凸函数，在使用梯度下降算法的时候，容易得到局部最优解，而不是全局最优解。因此要选择凸函数。\n",
    "\n",
    "首先最大似然函数:\n",
    "其中$h = P(y=1|x)$代表预测y==1的概率\n",
    "$$LL(h,y) = h^{y}(1-h)^(1-y)$$\n",
    "\n",
    "进行log后得到的对数似然损失函数如下:\n",
    "$$ll(h,y) = log(LL(h,y)) = ylog(h) + (1-y)log(1-h)$$\n",
    "\n",
    "最大似然函数最初是希望越大越好，求出估计参数，但是我们的损失函数是希望越小越好\n",
    "因此最大似然损失函数可以定义为-1 * 最大对数似然函数\n",
    "最大似然损失函数：\n",
    "\n",
    "$$l(h,y) = -1 * ll(h,y) = -ylog(h) - (1-y)log(1-h)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目标是最小化样本点的损失loss function,损失函数是针对单个样本点的\n",
    "\n",
    "#### cost function\n",
    "全部训练数据集的loss_function总和的平均值即为训练集的代价函数（cost function）\n",
    "$$J(w,b) = \\frac{1}{m}\\sum_{i=1}^{m}l(h^{(i)},y^{(i)}) = -\\frac{1}{m}[y^{(i)}logh^{(i)} + (1-y^{(i)})log(1-y^{(i)})]$$\n",
    "\n",
    "    1.cost function是带求系数w和b的函数\n",
    "    2.目标是迭代计算出最佳的w和b的值，最小化cost function，让其尽可能的接近于0\n",
    "    \n",
    "### 3.梯度下降\n",
    "用梯度下降法来最小化cost function,以计算出合适的w和b的值。\n",
    "每次迭代更新的修正的表达式:\n",
    "$$w := w - \\alpha\\frac{\\partial J(w,b)}{\\partial w}$$\n",
    "$$b := b - \\alpha\\frac{\\partial J(w,b)}{\\partial b}$$\n",
    "\n",
    "在程序代码中，通常使用dw表示$\\frac{\\partial J(w,b)}{\\partial w}$,db表示$\\frac{\\partial J(w,b)}{\\partial b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.逻辑回归中的梯度下降法\n",
    "对单个样本而言，逻辑回归loss function表达式为:\n",
    "$$z = w^{T}x + b$$\n",
    "$$h = \\sigma(z)$$\n",
    "$$l(h,y) = -ylog(h) - (1-y)log(1-h)$$\n",
    "\n",
    "反向传播过程\n",
    "$$dh = \\frac{\\partial l}{\\partial h} = -\\frac{y}{h} + \\frac{(1-y)}{(1-h)}$$\n",
    "$$dz=\\frac{\\partial l}{\\partial z}=\\frac{\\partial l)}{\\partial h} * \\frac{\\partial h}{\\partial z} = (-\\frac{y}{h} + \\frac{(1-y)}{(1-h)}) * h(1-h) = h-y$$\n",
    "$$dw_{1} = \\frac{\\partial l}{\\partial w_{1}} = \\frac{\\partial l}{\\partial z} * \\frac{\\partial z}{\\partial w_{1}} = x_{1}*(a-y)$$\n",
    "$$db = \\frac{\\partial l}{\\partial b} = \\frac{\\partial l}{\\partial z} * \\frac{\\partial z}{\\partial b} = a-y$$\n",
    "\n",
    "梯度下降法:\n",
    "$$w_{1} := w_{1} - \\alpha dw_{1}$$\n",
    "$$w_{2} := w_{2} - \\alpha dw_{2}$$\n",
    "$$b := b - \\alpha db$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.m个样本的梯度下降\n",
    "对m个样本来说，其cost function表达式如下:\n",
    "$$z^{i} = w^{T}x^{(i)} + b$$\n",
    "$$h^{(i)} = \\sigma(z^{i})$$\n",
    "$$J(w,b) = \\frac{1}{m}\\sum_{i=1}^{m}l(h^{(i)},y^{(i)}) = -\\frac{1}{m}[y^{(i)}logh^{(i)} + (1-y^{(i)})log(1-y^{(i)})]$$\n",
    "cost function 关于w和b的偏导数可以写成所有样本点偏导数和的平均形式:\n",
    "$$dw_{1} = \\frac{1}{m}\\sum_{i=1}^{m}x_{1}^{(i)}(h^{(i)} - y^{(i)})$$\n",
    "$$dw_{2} = \\frac{1}{m}\\sum_{i=1}^{m}x_{2}^{(i)}(h^{(i)} - y^{(i)})$$\n",
    "$$db = \\frac{1}{m}\\sum_{i=1}^{m}(h^{(i)} - y^{(i)})$$\n",
    "\n",
    "完整的logistic 回归中某次训练的流程如下，这里假设特征向量的维度为2\n",
    "\n",
    "\n",
    "$J = 0,dw_{1} = 0,dw_{2} = 0,db = 0$\n",
    "\n",
    "for i = 1 to m:\n",
    "\n",
    "&emsp;&emsp;$z^{(i)} = w^{T}x^{(i)} + b$\n",
    "\n",
    "&emsp;&emsp;$h^{(i)} = \\sigma(z^{(i)})$\n",
    "\n",
    "&emsp;&emsp;$J += -[y^{(i)}logh^{(i)} + (1-y^{(i)})log(1-y^{(i)})]$\n",
    "\n",
    "&emsp;&emsp;$dz^{(i)} = h^{(i)}(1-h^{(i)})$\n",
    "\n",
    "&emsp;&emsp;$dw_{1} += x_{1}^{(i)}dz^{(i)}$\n",
    "\n",
    "&emsp;&emsp;$dw_{2} += x_{2}^{(i)}dz^{(i)}$\n",
    "\n",
    "&emsp;&emsp;$db += dz^{(i)}$\n",
    "\n",
    "$J = J/m,dw_{1} = dw_{1}/m,dw_{2} = dw_{2}/m,db = db/m$\n",
    "\n",
    "然后对$w_{1},w_{2},b$进行迭代。\n",
    "\n",
    "上诉过程在计算的时候有一个缺点：需要编写两个for循环。第一个for循环遍历m个样本，第二个for循环遍历所有特征。如果有大量特征，在代码中显示使用for循环会使算法很低效，向量化可以用于解决显示使用for循环的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.向量化（Vectorization）\n",
    "在logistic 回归中，需要计算：\n",
    "$$z = w^{T}x + b$$\n",
    "如果是非向量化的循环方式操作，代码可能如下:\n",
    "```\n",
    "z = 0\n",
    "for i in range(n_x):\n",
    "    z += w[i] * x[i]\n",
    "z += b\n",
    "```\n",
    "#### 剖析如何去掉for循环\n",
    "\n",
    "$$dw_{1} = \\frac{1}{m}\\sum_{i=1}^{m}x_{1}^{(i)}(h^{(i)} - y^{(i)})$$\n",
    "$$dw_{2} = \\frac{1}{m}\\sum_{i=1}^{m}x_{2}^{(i)}(h^{(i)} - y^{(i)})$$\n",
    "$$db = \\frac{1}{m}\\sum_{i=1}^{m}(h^{(i)} - y^{(i)})$$\n",
    "\n",
    "其中$dw$是求导矩阵，可以将$dw_{1},dw_{2}$按照行进行拼接\n",
    "\n",
    "$$dw = \n",
    " \\left[\n",
    " \\begin{matrix}\n",
    "   dw_{1} \\\\\n",
    "   dw_{2} \\\\\n",
    "  \\end{matrix}\n",
    "  \\right]\n",
    "  = \\frac{1}{m}\n",
    "  \\left[\n",
    "  \\begin{matrix}\n",
    "   \\sum_{i=1}^{m}x_{1}^{(i)}(h^{(i)} - y^{(i)}) \\\\\n",
    "   \\sum_{i=1}^{m}x_{2}^{(i)}(h^{(i)} - y^{(i)}) \\\\\n",
    "  \\end{matrix}\n",
    "  \\right]\n",
    "$$\n",
    "\n",
    "重新定义向量\n",
    "\n",
    "1. 输入矩阵X: ($n_{x}$,m)\n",
    "2. 权重矩阵W: ($n_{x}$,1)\n",
    "3. 偏置b: 为一个常数\n",
    "4. 输出矩阵Y: (1,m)\n",
    "\n",
    "假设特征向量个数为2，然后有m个样本:\n",
    "输入矩阵X格式如下:\n",
    "$$X = \n",
    " \\left[\n",
    " \\begin{matrix}\n",
    "   x_1^{1} & \\cdots x_1^{m} \\\\\n",
    "   x_2^{1} & \\cdots x_2^{m} \\\\\n",
    "  \\end{matrix}\n",
    "  \\right]\n",
    "$$\n",
    "权重矩阵w格式如下:\n",
    "$$\n",
    "W = \n",
    " \\left[\n",
    " \\begin{matrix}\n",
    "   w_1 \\\\\n",
    "   w_2 \\\\\n",
    "  \\end{matrix}\n",
    "  \\right]\n",
    "$$\n",
    "那么累加和矩阵Z 格式如下:\n",
    "$$\n",
    "Z = \n",
    " \\left[\n",
    " \\begin{matrix}\n",
    "   z_1 & \\cdots z_m \\\\\n",
    "  \\end{matrix}\n",
    "  \\right]\n",
    "  =\n",
    "  \\left[\n",
    "  \\begin{matrix}\n",
    "   W^{T} * X^{(1)} + b & \\cdots W^{T} * X^{(m)} + b \\\\\n",
    "  \\end{matrix}\n",
    "  \\right]\n",
    "  = W^{T} * X + b\n",
    "$$\n",
    "\n",
    "\n",
    "如果是向量化操作，代码会简洁很多，并带来近百倍的性能提升(并行指令)\n",
    "```\n",
    "z = np.dot(w.T,x) + b\n",
    "h = sigmoid(z)\n",
    "```\n",
    "\n",
    "#### 逻辑回归梯度下降输出向量化\n",
    "不用显示for循环，实现logistic回归的梯度下降一次迭代(即计算m个样本的平均损失):\n",
    "$$Z = w^{T}X + b = np.dot(w.T,x) + b$$\n",
    "$$H = \\sigma(Z)$$\n",
    "$$dZ = A - Y$$\n",
    "$$dw = \\frac{1}{m}XdZ^{T}$$\n",
    "$$db = \\frac{1}{m}np.sum(dZ)$$\n",
    "$$w := w - \\alpha dw$$\n",
    "$$b := b - \\alpha db$$\n",
    "\n",
    "多次迭代的梯度下降依然需要for循环\n",
    "\n",
    "### 7.手动推导过程\n",
    "\n",
    "![hands](./picture/hands_logistic.jpg)\n",
    "\n",
    "\n",
    "### 8.代码\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "def create_data():\n",
    "    iris = load_iris()\n",
    "    df = pd.DataFrame(iris.data,columns = iris.feature_names)\n",
    "    df['label'] = iris.target\n",
    "    df.columns = ['sepal length', 'sepal width', 'petal length', 'petal width', 'label']\n",
    "    data = np.array(df.iloc[:100, [0,1,-1]])\n",
    "    # print(data)\n",
    "    return data[:,:2], data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
       "       0., 1.])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionClassifier:\n",
    "    def __init__(self,max_iter=2,learning_rate=0.005):\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        #self.X_norm,self.ranges,self.min_vals = self.auto_norm(self.X_train)\n",
    "        \n",
    "    # 归一化，自己写    \n",
    "    def auto_norm(self,data_set):\n",
    "        min_vals = data_set.min(0)\n",
    "        max_vals = data_set.max(0)\n",
    "        ranges = max_vals - min_vals\n",
    "        m,n = data_set.shape\n",
    "        norm_data = data_set - np.tile(min_vals,(m,1))\n",
    "        norm_data = norm_data/np.tile(ranges,(m,1))\n",
    "        return norm_data,ranges,min_vals\n",
    "    \n",
    "    def correct_data(self,data_set):\n",
    "        '''\n",
    "        在归一化测试数据集的时候，可能出现异常情况\n",
    "        '''\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        s = 1./(1 + np.exp(-x))\n",
    "        return s\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        '''\n",
    "        注意此时X.shape=(m,n) Y = (m,1)\n",
    "        '''\n",
    "        m,n = X.shape\n",
    "        X = X.reshape((n,m))\n",
    "        print('X.shape ' + str(X.shape))\n",
    "        Y = y.reshape((1,m))\n",
    "        self.weights = np.array([0.0] * n).reshape((n,1))\n",
    "        #print('weights.shape ' + str(self.weights.shape))\n",
    "        \n",
    "        self.bias = 0.0\n",
    "        for iter in range(self.max_iter):\n",
    "            Z = np.dot(self.weights.T,X) + self.bias\n",
    "            #print('Z.shape ' + str(Z.shape))\n",
    "            H = self.sigmoid(Z)\n",
    "            dz = H - Y\n",
    "            #print('dz.shape ' + str(dz.shape))\n",
    "            dw = (1./m) * np.dot(X,dz.T)\n",
    "            #print('dz ' + str(dz))\n",
    "            db = (1./m) * np.sum(dz,axis=1)\n",
    "            #print('db ' + str(db))\n",
    "            #print('dw.shape ' + str(dw.shape))\n",
    "            self.weights = self.weights - self.learning_rate * dw\n",
    "            self.bias = self.bias - self.learning_rate * db\n",
    "        print('LogisticRegression Model(learning_rate={},max_iter={})'.format(\n",
    "            self.learning_rate, self.max_iter))\n",
    "    def score(self,X_test,y_test):\n",
    "        '''\n",
    "        输入矩阵X_test.shape [m,n_x]\n",
    "        输入labels Array[y1,y2]\n",
    "        '''\n",
    "        m,n = X_test.shape\n",
    "        X_test = X_test.reshape((n,m))\n",
    "        X_pred = np.dot(self.weights.T,X_test) + self.bias\n",
    "        X_pred = X_pred.reshape((m,))\n",
    "        right_count = 0\n",
    "        for i in range(m):\n",
    "            if (y_test[i] == 1 and X_pred[i] >= 0) or (y_test[i] == 0 and X_pred[i] < 0):\n",
    "                right_count += 1\n",
    "        return right_count/m\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape (2, 70)\n",
      "LogisticRegression Model(learning_rate=0.005,max_iter=2)\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegressionClassifier()\n",
    "lr_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43333333333333335"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01932845258568052"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.weights[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05036773])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70,)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a1c66a940>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_points = np.arange(4, 8)\n",
    "y_ = -(lr_clf.weights[0,0] * x_points + lr_clf.bias)/lr_clf.weights[1,0]\n",
    "plt.plot(x_points,y_)\n",
    "\n",
    "plt.scatter(X_train[:50,0],X[:50,1], label='0')\n",
    "plt.scatter(X[50:,0],X[50:,1], label='1')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 感觉上面的代码没问题，但是正确率太低了，感觉哪里不对劲，重新写一个类\n",
    "class LRClassifier:\n",
    "    def __init__(self,max_iter=500,learning_rate=0.5,print_cost=True):\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.print_cost = print_cost\n",
    "        \n",
    "    def sigmoid(self,z):\n",
    "        s = 1./(1 + np.exp(-z))\n",
    "        return s\n",
    "    \n",
    "    def initialize_with_zeros(self,dim):\n",
    "        w = np.ones(shape=(dim,1),dtype=np.float32)\n",
    "        b = 1.0\n",
    "        assert(w.shape == (dim,1))\n",
    "        assert(isinstance(b,float))\n",
    "        return w,b\n",
    "    \n",
    "    def propagate(self,w,b,X,Y):\n",
    "        m = X.shape[1]\n",
    "        H = self.sigmoid(np.dot(w.T,X) + b)\n",
    "        assert(H.shape == (1,m))\n",
    "\n",
    "        cost = (-1.0/m) * np.sum((Y * np.log(H)) + (1-Y) * np.log(1-H),axis=1)\n",
    "        dw = (1.0/m) * np.dot(X,(H-Y).T)\n",
    "        db = (1.0/m) * np.sum(H-Y)\n",
    "        #print('db ' + str(db))\n",
    "        #print('(H-Y).T.shape ' + str((H-Y).T.shape))\n",
    "        #print('dw.shape ' + str(dw.shape))\n",
    "        #print('w.shape' + str(w.shape))\n",
    "        assert(dw.shape == w.shape)\n",
    "        assert(isinstance(db,float))\n",
    "        cost = np.squeeze(cost)\n",
    "        assert(cost.shape == ())\n",
    "        \n",
    "        grads = {'dw':dw,'db':db}\n",
    "        return grads,cost\n",
    "    \n",
    "    def optimize(self,w,b,X,Y):\n",
    "        costs = []\n",
    "        for i in range(self.max_iter):\n",
    "            grads,cost = self.propagate(w,b,X,Y)\n",
    "            dw = grads['dw']\n",
    "            db = grads['db']\n",
    "            \n",
    "            w = w - self.learning_rate * dw\n",
    "            b = b - self.learning_rate * db\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                costs.append(cost)\n",
    "            if self.print_cost and i % 100 == 0:\n",
    "                print('Cost after iteration %d: %f '%(i,cost))\n",
    "        params = {'w':w,'b':b}\n",
    "        grads = {'dw':dw,'db':db}\n",
    "        \n",
    "        return params,grads,costs\n",
    "    \n",
    "    def predict(self,w,b,X):\n",
    "        m = X.shape[1]\n",
    "        Y_prediction = np.zeros((1,m))\n",
    "        w = w.reshape((X.shape[0],1))\n",
    "        \n",
    "        H = self.sigmoid(np.dot(w.T,X) + b)\n",
    "        for i in range(H.shape[1]):\n",
    "            if H[0,i] >= 0.5:\n",
    "                Y_prediction[0,i] = 1\n",
    "            else:\n",
    "                Y_prediction[0,i] = 0\n",
    "        assert(Y_prediction.shape == (1,m))\n",
    "        \n",
    "        return Y_prediction\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        '''\n",
    "        矩阵格式需要转换，一开始是X :[m,n] 需要转换为[n,m]\n",
    "        '''\n",
    "        m,n = X.shape\n",
    "        X = X.reshape((n,m))\n",
    "        y = y.reshape((1,m))\n",
    "        #print('X.shape ' + str(X.shape))\n",
    "        #print('y.shape ' + str(y.shape))\n",
    "        \n",
    "        self.w,self.b = self.initialize_with_zeros(n)\n",
    "        params,grads,costs = self.optimize(self.w,self.b,X,y)\n",
    "        self.w = params['w']\n",
    "        self.b = params['b']\n",
    "        \n",
    "        y_prediction = self.predict(self.w,self.b,X)\n",
    "        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction - y)) * 100))\n",
    "        d = {'costs':costs,\n",
    "             'y_prediction':y_prediction,\n",
    "             'w':self.w,\n",
    "             'b':self.b,\n",
    "             'learning_rate':self.learning_rate,\n",
    "             'max_iter':self.max_iter\n",
    "            }\n",
    "        return d\n",
    "    \n",
    "    def score(self,X_test,y_test):\n",
    "        m,n = X_test.shape\n",
    "        X_test = X_test.reshape((n,m))\n",
    "        y_test = y_test.reshape((1,m))\n",
    "        y_prediction = self.predict(self.w,self.b,X_test)\n",
    "        print('y_prediction ' + str(y_prediction))\n",
    "        print('y_test ' + str(y_test))\n",
    "        return (1 - np.mean(np.abs(y_prediction - y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 5.007622 \n",
      "Cost after iteration 100: 1.544078 \n",
      "Cost after iteration 200: 1.589089 \n",
      "Cost after iteration 300: 1.628537 \n",
      "Cost after iteration 400: 1.656166 \n",
      "train accuracy: 47.14285714285714 %\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LRClassifier()\n",
    "d = lr_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_prediction [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "y_test [[0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1.\n",
      "  0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      "  0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "res = lr_clf.score(X_train,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4714285714285714"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a1cd7db00>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_points = np.arange(4, 8)\n",
    "y_ = -(lr_clf.w[0,0] * x_points + lr_clf.b)/lr_clf.w[1,0]\n",
    "plt.plot(x_points,y_)\n",
    "\n",
    "plt.scatter(X[:50,0],X[:50,1], label='0')\n",
    "plt.scatter(X[50:,0],X[50:,1], label='1')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "我分析准确率差这么多的原因\n",
    "1.首先参考代码是采用随机梯度下降法，每次更新一个实例\n",
    "2.发现一个简单问题，最后更新的时候损失不在下降\n",
    "'''\n",
    "class LogisticReressionClassifier:\n",
    "    def __init__(self, max_iter=200, learning_rate=0.01):\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + exp(-x))\n",
    "\n",
    "    def data_matrix(self, X):\n",
    "        data_mat = []\n",
    "        for d in X:\n",
    "            data_mat.append([1.0, *d])\n",
    "        return data_mat\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # label = np.mat(y)\n",
    "        data_mat = self.data_matrix(X)  # m*n\n",
    "        self.weights = np.zeros((len(data_mat[0]), 1), dtype=np.float32)\n",
    "\n",
    "        for iter_ in range(self.max_iter):\n",
    "            for i in range(len(X)):\n",
    "                result = self.sigmoid(np.dot(data_mat[i], self.weights))\n",
    "                error = y[i] - result\n",
    "                self.weights += self.learning_rate * error * np.transpose(\n",
    "                    [data_mat[i]])\n",
    "        print('LogisticRegression Model(learning_rate={},max_iter={})'.format(\n",
    "            self.learning_rate, self.max_iter))\n",
    "\n",
    "    # def f(self, x):\n",
    "    #     return -(self.weights[0] + self.weights[1] * x) / self.weights[2]\n",
    "\n",
    "    def score(self, X_test, y_test):\n",
    "        right = 0\n",
    "        X_test = self.data_matrix(X_test)\n",
    "        for x, y in zip(X_test, y_test):\n",
    "            result = np.dot(x, self.weights)\n",
    "            if (result > 0 and y == 1) or (result < 0 and y == 0):\n",
    "                right += 1\n",
    "        return right / len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression Model(learning_rate=0.01,max_iter=200)\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticReressionClassifier()\n",
    "lr_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9857142857142858"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
